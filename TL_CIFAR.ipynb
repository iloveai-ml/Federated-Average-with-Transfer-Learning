{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9155dcb",
   "metadata": {},
   "source": [
    "# Transfer Learning using CIFAR-10 dataset\n",
    "\n",
    "#### Name Origin: The Canadian Institute for Advanced Research (CIFAR) is a Canadian-based global research organization that brings together teams of top researchers from around the world to address important and complex questions. \n",
    "The CIFAR-10 dataset is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0882a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b965489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 10)\n",
      "x_test shape: (10000, 32, 32, 3), y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the shapes of the loaded data and one-hot encoded labels\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79c0aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 187s 2us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20292170 (77.41 MB)\n",
      "Trainable params: 267786 (1.02 MB)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        # Load the pre-trained VGG19 model\n",
    "        base_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
    "        # Freeze all layers except the last two convolutional layers and the classification layer\n",
    "#         for layer in base_model.layers[:-5]:\n",
    "#             layer.trainable = False\n",
    "        base_model.trainable=False\n",
    "        # Create the transfer learning model by adding custom classification layers on top of the base model\n",
    "        model2 = tf.keras.models.Sequential([\n",
    "            base_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')  # Adjust the number of output classes accordingly\n",
    "        ])\n",
    "'''Function: It calculates the average value for each feature map across the spatial dimensions (height and width), effectively reducing each feature map\n",
    "to a single average value.\n",
    "Input Shape: The expected input is a 4D tensor with shape (batch_size, height, width, channels) if data_format='channels_last', or (batch_size, channels, height, width) if data_format='channels_first'.\n",
    "Output Shape: The output is a 2D tensor with shape (batch_size, channels). If the keepdims argument is set to True, the output will retain the reduced \n",
    "spatial dimensions with length 1, resulting in a 4D tensor.\n",
    "Usage: This layer is often used in CNNs before the final classification layer to reduce the spatial dimensions of the input and to minimize overfitting\n",
    "by focusing on the most important features.'''\n",
    "\n",
    "        # Create an optimizer with the specified learning rate\n",
    "        custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "        # Compile the model with the custom optimizer\n",
    "        model2.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model2\n",
    "    \n",
    "# Create an instance of the SimpleMLP model\n",
    "simple_mlp_model = SimpleMLP.build()\n",
    "global_model=simple_mlp_model\n",
    "\n",
    "# Display the model summary\n",
    "simple_mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd232f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client1: 50000 images\n",
      "client2: 100000 images\n",
      "client3: 150000 images\n",
      "client4: 200000 images\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Number of clients\n",
    "num_clients = 4\n",
    "\n",
    "# Initialize data structures for each client\n",
    "client_data = {f'client{i + 1}': {'images': [], 'labels': []} for i in range(num_clients)}\n",
    "\n",
    "# Define the distribution of classes among clients (modify as needed)\n",
    "class_distribution = [0.1, 0.2, 0.3, 0.4]  # Adjust the percentages based on your requirements\n",
    "\n",
    "# Ensure the distribution sums to 1\n",
    "class_distribution = np.array(class_distribution) / sum(class_distribution)\n",
    "\n",
    "# Split the data into clients based on the defined distribution\n",
    "for label in np.unique(y_train):\n",
    "    label_indices = np.where(y_train == label)[0]\n",
    "    \n",
    "    # Shuffle indices for randomness\n",
    "    np.random.shuffle(label_indices)\n",
    "\n",
    "    # Distribute indices to clients based on the specified distribution\n",
    "    cumulative_distribution = np.cumsum(class_distribution)\n",
    "    for i in range(num_clients):\n",
    "        start_index = int(cumulative_distribution[i - 1] * len(label_indices)) if i > 0 else 0\n",
    "        end_index = int(cumulative_distribution[i] * len(label_indices))\n",
    "        client_indices = label_indices[start_index:end_index]\n",
    "\n",
    "        # Assign images and labels to the client\n",
    "        client_data[f'client{i + 1}']['images'].extend(x_train[client_indices])\n",
    "        client_data[f'client{i + 1}']['labels'].extend(y_train[client_indices])\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "for client_id in client_data.keys():\n",
    "    client_data[client_id]['images'] = np.array(client_data[client_id]['images'])\n",
    "    client_data[client_id]['labels'] = np.array(client_data[client_id]['labels'])\n",
    "\n",
    "# # One-hot encode labels\n",
    "# client_data[client_id]['one_hot_labels'] = tf.keras.utils.to_categorical(client_data[client_id]['labels'], num_classes=10)\n",
    "\n",
    "# Display the number of images for each client\n",
    "for client_id, data in client_data.items():\n",
    "    print(f\"{client_id}: {len(data['images'])} images\")\n",
    "\n",
    "\n",
    "# Assuming you have client_data and each client's labels\n",
    "label1 = client_data['client1']['labels']\n",
    "label2 = client_data['client2']['labels']\n",
    "label3 = client_data['client3']['labels']\n",
    "label4 = client_data['client4']['labels']\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10 \n",
    "one_hot_label1 = to_categorical(label1, num_classes=num_classes)\n",
    "one_hot_label2 = to_categorical(label2, num_classes=num_classes)\n",
    "one_hot_label3 = to_categorical(label3, num_classes=num_classes)\n",
    "one_hot_label4 = to_categorical(label4, num_classes=num_classes)\n",
    "\n",
    "# Now, 'one_hot_labels' can be added to the client_data dictionary\n",
    "\n",
    "client_data['client1']['one_hot_labels'] = one_hot_label1\n",
    "client_data['client2']['one_hot_labels'] = one_hot_label2\n",
    "client_data['client3']['one_hot_labels'] = one_hot_label3\n",
    "client_data['client4']['one_hot_labels'] = one_hot_label4\n",
    "   \n",
    "train1 = client_data['client1']['images']\n",
    "train2 = client_data['client2']['images']\n",
    "train3 = client_data['client3']['images']\n",
    "train4 = client_data['client4']['images']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5283c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(data_dict):\n",
    "    '''\n",
    "    Return a dictionary with keys as client names and values as data and label lists.\n",
    "    Args: data_dict: A dictionary where keys are client names, and values are tuples of data and labels.\n",
    "                    For example, {'client_1': (data_1, labels_1), 'client_2': (data_2, labels_2), ...}\n",
    "    Returns: A dictionary with keys as client names and values as tuples of data and label lists.\n",
    "    '''\n",
    "    return data_dict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def test_model(x_test, y_test,  model, comm_round):\n",
    "    loss,accuracy=model.evaluate(x_test, y_test)\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, accuracy, loss))\n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def avg_weights(scaled_weight_list):\n",
    "    '''Return the average of the listed scaled weights.'''\n",
    "    num_clients = len(scaled_weight_list)\n",
    "\n",
    "    if num_clients == 0:\n",
    "        return None  # Handle the case where the list is empty\n",
    "\n",
    "    avg_grad = list()\n",
    "\n",
    "    # Get the sum of gradients across all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0) / num_clients\n",
    "        avg_grad.append(layer_mean)\n",
    "\n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "client_data = {\n",
    "    'client_1': (train1,label1),\n",
    "    'client_2': (train2,label2),\n",
    "    'client_3': (train3,label3),\n",
    "    'client_4': (train4,label4)\n",
    "    \n",
    "}\n",
    "\n",
    "#create clients\n",
    "clients_batched = create_clients(client_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77169dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_names = list(clients_batched.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed3e0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress Bar:   0%|                                                                              | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "6250/6250 - 491s - loss: 1.2722 - accuracy: 0.5506 - 491s/epoch - 79ms/step\n",
      "Epoch 2/2\n",
      "6250/6250 - 489s - loss: 1.0675 - accuracy: 0.6241 - 489s/epoch - 78ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress Bar:  25%|█████████████████                                                   | 1/4 [16:45<50:17, 1005.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9375/9375 - 716s - loss: 1.2196 - accuracy: 0.5691 - 716s/epoch - 76ms/step\n",
      "Epoch 2/2\n",
      "9375/9375 - 700s - loss: 0.9981 - accuracy: 0.6473 - 700s/epoch - 75ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress Bar:  50%|██████████████████████████████████                                  | 2/4 [40:51<42:09, 1264.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3125/3125 - 230s - loss: 1.3541 - accuracy: 0.5225 - 230s/epoch - 74ms/step\n",
      "Epoch 2/2\n",
      "3125/3125 - 236s - loss: 1.1572 - accuracy: 0.5917 - 236s/epoch - 75ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress Bar:  75%|███████████████████████████████████████████████████▊                 | 3/4 [48:46<15:03, 903.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "12500/12500 - 951s - loss: 1.1758 - accuracy: 0.5857 - 951s/epoch - 76ms/step\n",
      "Epoch 2/2\n",
      "12500/12500 - 941s - loss: 0.9383 - accuracy: 0.6694 - 941s/epoch - 75ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Bar: 100%|██████████████████████████████████████████████████████████████████| 4/4 [1:21:24<00:00, 1221.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 38s 119ms/step - loss: 1.2049 - accuracy: 0.5935\n",
      "comm_round: 0 | global_acc: 59.350% | global_loss: 1.2049115896224976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress Bar:   0%|                                                                              | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3125/3125 - 235s - loss: 1.0199 - accuracy: 0.6385 - 235s/epoch - 75ms/step\n",
      "Epoch 2/2\n",
      "3125/3125 - 232s - loss: 0.9246 - accuracy: 0.6722 - 232s/epoch - 74ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Bar:  25%|█████████████████▎                                                   | 1/4 [08:47<26:23, 527.76s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.29 GiB for an array with shape (200000, 32, 32, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m\n\u001b[0;32m     47\u001b[0m         local_model\u001b[38;5;241m.\u001b[39mset_weights(global_weights)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#         generator = DataGenerator(clients_batched[client][0], clients_batched[client][1], batch_size=15)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#         # Fit local model with client's data using generator\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \n\u001b[0;32m     58\u001b[0m         \u001b[38;5;66;03m# Fit local model with client's data\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m         local_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     60\u001b[0m             np\u001b[38;5;241m.\u001b[39marray(clients_batched[client][\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m     61\u001b[0m             np\u001b[38;5;241m.\u001b[39marray(clients_batched[client][\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m     62\u001b[0m             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     63\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     64\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     65\u001b[0m         )\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# Get the scaled model weights and add to the list\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         weights \u001b[38;5;241m=\u001b[39m local_model\u001b[38;5;241m.\u001b[39mget_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.29 GiB for an array with shape (200000, 32, 32, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# from keras.utils import Sequence\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "# from keras import backend as K\n",
    "\n",
    "comms_round = 30  # Number of global epochs\n",
    "acc3 = []\n",
    "\n",
    "# class DataGenerator(Sequence):\n",
    "#     def __init__(self, data, labels, batch_size):\n",
    "#         self.data = data\n",
    "#         self.labels = labels\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         batch_data = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "#         batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "#         return np.array(batch_data), np.array(batch_labels)\n",
    "\n",
    "for comm_round in range(comms_round):\n",
    "\n",
    "    # Get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "\n",
    "    # Initial list to collect local model weights after scaling\n",
    "    local_weight_list = []\n",
    "\n",
    "    # Randomize client data - using keys\n",
    "    client_names = list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "\n",
    "    for client in tqdm(client_names, desc='Progress Bar'):\n",
    "        local_model = SimpleMLP.build()\n",
    "\n",
    "        local_model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "#         generator = DataGenerator(clients_batched[client][0], clients_batched[client][1], batch_size=15)\n",
    "\n",
    "#         # Fit local model with client's data using generator\n",
    "#         local_model.fit(\n",
    "#             generator,\n",
    "#             epochs=2,\n",
    "#             verbose=2\n",
    "#         )\n",
    "        \n",
    "        # Fit local model with client's data\n",
    "        local_model.fit(\n",
    "            np.array(clients_batched[client][0]),\n",
    "            np.array(clients_batched[client][1]),\n",
    "            epochs=2,\n",
    "            batch_size= 8,\n",
    "            verbose= 2\n",
    "        )\n",
    "\n",
    "        # Get the scaled model weights and add to the list\n",
    "        weights = local_model.get_weights()\n",
    "        local_weight_list.append(weights)\n",
    "\n",
    "        # Clear the session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "\n",
    "    # Calculate the average weights across all clients for each layer\n",
    "    average_weights = avg_weights(local_weight_list)\n",
    "\n",
    "    # Update the global model with the average weights\n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    # Optionally, you can also test the global model at this point using a separate test dataset\n",
    "    global_acc, global_loss = test_model(x_test, y_test, global_model, comm_round)\n",
    "    acc3.append(global_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20dca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc3=np.array(acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c612c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"acc_fedavg_cifar.npy\",acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.save(\"fedavg_cifar.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f63f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"acc_fedavg_cifar.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4189767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
